{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that linear response theory is recovering the true amplification of freshwater fluxes, we need to evaluate the amplification of fluxes from CESM flux fields. Note that as linear response theory is finding the change in freshwater fluxes as a proportion of the FAFMIP perturbation, the truth is not exactly the change in magnitude of freshwater fluxes, but rather the change of the projection of the freshwater fluxes onto the FAFMIP pattern. There are other ways that we have thought of to quantify the true fluxes which are shown in the \"true_freshwater_fluxes_options.ipynb\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import netCDF4\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os, glob \n",
    "import imageio\n",
    "from matplotlib import animation\n",
    "import copy\n",
    "import cartopy as cart\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter #see https://scitools.org.uk/cartopy/docs/v0.15/examples/tick_labels.html\n",
    "import certifi\n",
    "import ssl\n",
    "import math\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "from scipy import stats\n",
    "from xgcm import Grid\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.axes._secondary_axes import SecondaryAxis\n",
    "import xesmf as xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, load in all the freshwater fluxes. See notebook processing_salt_fluxes.py for how to generate this files that we unpickle here\n",
    "import pickle \n",
    "\n",
    "with open(\"/scratch/abf376/regridded_salt_flux_2005on\", \"rb\") as fp:   #Unpickling\n",
    "    regridded_2005on=pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we calculate the true fluxes for the ensemble mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average over ensemble member\n",
    "salt_flux_avg_2005on=sum(regridded_2005on)/34\n",
    "salt_flux_avg_2005on=salt_flux_avg_2005on.rename({'y': 'latitude','x': 'longitude'})\n",
    "salt_flux_avg_2005on=salt_flux_avg_2005on.assign_coords(latitude=salt_flux_avg_2005on.lat[:,0],longitude=salt_flux_avg_2005on.lon[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "salt_flux_avg_2006to2055=salt_flux_avg_2005on[0:12*59,:,:] #salt_avg was from 1920 to 2080 so this is from 1970 to 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create area grid\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '/scratch/abf376/')\n",
    "from area_grid import *\n",
    "\n",
    "area=area_grid(latitudes=np.array(salt_flux_avg_2005on[0,:,:].latitude),longitudes=salt_flux_avg_2005on[0,:,:].longitude)\n",
    "area=xr.DataArray(area,dims=[\"latitude\",\"longitude\"],coords=[salt_flux_avg_2005on[0,:,:].latitude,salt_flux_avg_2005on[0,:,:].longitude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#water flux from fafmip\n",
    "f='/scratch/abf376/FAFMIP_wfo_v2.nc' #this is the first 50 years\n",
    "file2read = netCDF4.Dataset(f,'r')\n",
    "#print(file2read.variables)\n",
    "wfo = xr.open_dataset(f)['water_flux_into_sea_water']\n",
    "wfo=wfo.where(wfo<1E19)\n",
    "\n",
    "\n",
    "area_wfo=area_grid(latitudes=np.array(wfo.latitude),longitudes=wfo.longitude)\n",
    "area_wfo=xr.DataArray(area,dims=[\"latitude\",\"longitude\"],coords=[wfo.latitude,wfo.longitude])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.9/site-packages/xarray/core/dataarray.py:780: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return key in self.data\n"
     ]
    }
   ],
   "source": [
    "#regrid wfo\n",
    "ds_out = xe.util.grid_global(1, 1)\n",
    "regridder_wfo= xe.Regridder(wfo, ds_out, \"bilinear\",periodic=True)\n",
    "wfo = regridder_wfo(wfo)\n",
    "wfo=wfo.rename({'y': 'latitude','x': 'longitude'})\n",
    "wfo=wfo.assign_coords(latitude=wfo.lat[:,0],longitude=wfo.lon[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to compute the projection of cesm fluxes onto the fafmip perturbation field\n",
    "\n",
    "y=np.array(np.reshape(wfo.mean('time')*area.where(area.latitude<65),(1,180*360))) #reshape the perturbation field to be a vector\n",
    "\n",
    "proj=np.empty(45)\n",
    "err=np.empty(45)\n",
    "for i in range(0,45):\n",
    "    x=np.array(np.reshape((salt_flux_avg_2006to2055[(i+5)*12:(i+5)*12,:,:].mean('time'))*area.where(area.latitude<65),(1,180*360)))\n",
    "    proj[i]=np.nansum(x*y)/(np.nansum(y*y))\n",
    "    err[i]=(np.nansum(np.abs(proj[i]*y)))/(np.nansum(np.abs(x-proj[i]*y))) #portion of cesm salt flux explained by projection divided by proportion of salt flux explained by rejection. come back to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.949971794685712e-01\n",
      "0.06445565332414613\n"
     ]
    }
   ],
   "source": [
    "# do block bootstrapping to deal with effect of natural variability \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "change=np.empty([3000])\n",
    "trend=np.empty([45])\n",
    "p=scipy.stats.linregress(np.linspace(0,44,45), y=proj, alternative='two-sided')\n",
    "trend=p.intercept+p.slope*np.linspace(0,44,45)\n",
    "\n",
    "from recombinator.block_bootstrap import circular_block_bootstrap\n",
    "\n",
    "# number of replications for bootstraps (number of resampled time-series to generate)\n",
    "B = 3000\n",
    "\n",
    "y_star_cb \\\n",
    "    = circular_block_bootstrap(proj-trend, \n",
    "                               block_length=2, \n",
    "                               replications=B, replace=True)\n",
    "bootstrap=[]\n",
    "for i in range(0,B):\n",
    "    bootstrap.append(trend+y_star_cb[i,:])\n",
    "\n",
    "for i in range(0,B):\n",
    "    change[i]=(bootstrap[i][40:45].mean()-bootstrap[i][0:5].mean())\n",
    "print(change.mean())\n",
    "print(change.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the change in the pattern strength\n",
    "z1=np.empty(50)\n",
    "for i in range(0,50):\n",
    "    z1[i]=np.nansum(np.array(np.reshape((np.abs(salt_flux_avg_2006to2055[i*12:(i+1)*12,:,:].mean('time')-salt_flux_avg_2006to2055[0*12:(5)*12,:,:].mean('time')))*area.where(area.latitude<65),(1,180*360))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "736260754.5442837\n",
      "87605581.97162563\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "changez=np.empty([500])\n",
    "trend=np.empty([45])\n",
    "p=scipy.stats.linregress(np.linspace(0,44,45), y=z1[5:50], alternative='two-sided')\n",
    "trend=p.intercept+p.slope*np.linspace(0,44,45)\n",
    "\n",
    "from recombinator.block_bootstrap import circular_block_bootstrap\n",
    "\n",
    "# number of replications for bootstraps (number of resampled time-series to generate)\n",
    "B = 500\n",
    "\n",
    "y_star_cb \\\n",
    "    = circular_block_bootstrap(z1[5:50]-trend, \n",
    "                               block_length=2, \n",
    "                               replications=B, replace=True)\n",
    "bootstrap=[]\n",
    "for i in range(0,B):\n",
    "    bootstrap.append(trend+y_star_cb[i,:])\n",
    "\n",
    "for i in range(0,B):\n",
    "    changez[i]=(bootstrap[i][42:45].mean()-bootstrap[i][0:3].mean())\n",
    "\n",
    "print(changez.mean())\n",
    "print(changez.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5378149853597608"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=np.array(np.reshape((salt_flux_avg_2006to2055[45*12:(50)*12,:,:].mean('time')-salt_flux_avg_2006to2055[5*12:(10)*12,:,:].mean('time'))*area.where(area.latitude<65),(1,180*360)))\n",
    "np.sqrt(np.nansum((change.mean()*y)**2))/np.sqrt(np.nansum((z)**2)) #near the end of the timeseries, the projection is about half the length of the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinregressResult(slope=1.312149732294195, intercept=-1321.8421978339602, rvalue=0.5761616913882781, pvalue=0.0, stderr=0.00998101518200372, intercept_stderr=237.26210064902753)\n"
     ]
    }
   ],
   "source": [
    "mask = ~np.isnan(change.mean()*y) & ~np.isnan(s)\n",
    "p=scipy.stats.linregress((change.mean()*y)[mask], s[mask], alternative='two-sided')\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinregressResult(slope=0.7084016093551855, intercept=-2250.817179072021, rvalue=0.7089943027702471, pvalue=0.0, stderr=0.003761169010840046, intercept_stderr=206.9531861217974)\n"
     ]
    }
   ],
   "source": [
    "l=s\n",
    "l=np.where(y<0,l,np.abs(l))\n",
    "l=np.where(y>0,l,-np.abs(l))\n",
    "\n",
    "mask = ~np.isnan(l) & ~np.isnan(s)\n",
    "p=scipy.stats.linregress(l[mask], s[mask], alternative='two-sided')\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the integration way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_pattern_wfo=np.empty(50)\n",
    "for i in range(0,50):\n",
    "    EP_pattern_wfo[i]=np.abs((((salt_flux_avg_2006to2055[i*12:(i+1)*12,:,:].mean('time')).where(np.sign(wfo.mean('time'))>0))*area.where(area.latitude<65)).sum())+np.abs((((salt_flux_avg_2006to2055[i*12:(i+1)*12,:,:].mean('time')).where(np.sign(wfo.mean('time'))<0))*area.where(area.latitude<65)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7697617409610814\n",
      "0.06405399721172099\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "A=((np.abs(wfo.mean('time')))*area.where(area.latitude<65)).sum() #strength of wfo in this way, 9.99660706e+08 if calculated with regridded or 1.08156723e+09 if not calculated with regridded\n",
    "\n",
    "\n",
    "change2=np.empty([3000])\n",
    "trend=np.empty([45])\n",
    "p=scipy.stats.linregress(np.linspace(0,44,45), y=EP_pattern_wfo[5:50], alternative='two-sided')\n",
    "trend=p.intercept+p.slope*np.linspace(0,44,45)\n",
    "\n",
    "from recombinator.block_bootstrap import circular_block_bootstrap\n",
    "\n",
    "# number of replications for bootstraps (number of resampled time-series to generate)\n",
    "B = 3000\n",
    "\n",
    "y_star_cb \\\n",
    "    = circular_block_bootstrap(EP_pattern_wfo[5:50]-trend, \n",
    "                               block_length=2, \n",
    "                               replications=B, replace=True)\n",
    "bootstrap=[]\n",
    "for i in range(0,B):\n",
    "    bootstrap.append(trend+y_star_cb[i,:])\n",
    "\n",
    "for i in range(0,B):\n",
    "    change2[i]=(bootstrap[i][42:45].mean()-bootstrap[i][0:3].mean())/A\n",
    "\n",
    "print(change2.mean())\n",
    "print(change2.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we need to do the same for each individual member. First, the projection technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of all ensemble members\n",
    "for i in range(0,34):\n",
    "    regridded_2005on[i]=regridded_2005on[i].rename({'y': 'latitude','x': 'longitude'})\n",
    "    regridded_2005on[i]=regridded_2005on[i].assign_coords(latitude=salt_flux_avg_2005on.latitude,longitude=salt_flux_avg_2005on.longitude)\n",
    "    \n",
    "salt_avg_list=[]\n",
    "for i in range(0,34):\n",
    "    salt_avg_list.append(regridded_2005on[i])\n",
    "    \n",
    "climatological_salt_flux_list=[]\n",
    "for i in range(0,34):\n",
    "    climatological_salt_flux_list.append(salt_avg_list[i][0:12*50,:,:].mean('time')) #this is 1920 to 1975\n",
    "    salt_avg_list[i]=salt_avg_list[i][0:12*50,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_list=np.empty([34,50])\n",
    "for j in range(0,34):\n",
    "    for i in range(0,50):\n",
    "        x=np.reshape(salt_avg_list[j][i*12:(i+1)*12,:,:].mean('time')*area.where(area.latitude<65),(1,180*360))\n",
    "        proj_list[j,i]=np.nansum(x*y)/(np.nansum(y*y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_store=np.empty(34)\n",
    "np.random.seed(0)\n",
    "change=np.empty([3000,34])\n",
    "for j in range(0,34):\n",
    "    trend=np.empty([45])\n",
    "    p=scipy.stats.linregress(np.linspace(0,44,45), y=proj_list[j,5:50], alternative='two-sided')\n",
    "    trend=p.intercept+p.slope*np.linspace(0,44,45)\n",
    "    pval_store[j]=p.pvalue\n",
    "\n",
    "    from recombinator.block_bootstrap import circular_block_bootstrap\n",
    "\n",
    "    # number of replications for bootstraps (number of resampled time-series to generate)\n",
    "    B = 3000\n",
    "\n",
    "    y_star_cb \\\n",
    "        = circular_block_bootstrap(proj_list[j,5:50]-trend, \n",
    "                                   block_length=2, \n",
    "                                   replications=B, replace=True)\n",
    "    bootstrap=[]\n",
    "    for i in range(0,B):\n",
    "        bootstrap.append(trend+y_star_cb[i,:])\n",
    "\n",
    "    for i in range(0,B):\n",
    "        change[i,j]=(bootstrap[i][40:45].mean()-bootstrap[i][0:5].mean())\n",
    "\n",
    "\n",
    "mean_boot=np.empty([34])\n",
    "std_boot=np.empty([34])\n",
    "for j in range(0,34):\n",
    "    mean_boot[j]=change[:,j].mean()\n",
    "    std_boot[j]=change[:,j].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'change_ensemble' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreshwater_fluxes_projection_pval_lin_regress_2011to2055_new\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:   \u001b[38;5;66;03m#Pickling\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(pval_store, fp)\n\u001b[0;32m----> 9\u001b[0m ensemble_projection_mean_std\u001b[38;5;241m=\u001b[39m[\u001b[43mchange_ensemble\u001b[49m\u001b[38;5;241m.\u001b[39mmean(),change_ensemble\u001b[38;5;241m.\u001b[39mstd()]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfreshwater_fluxes_ensemble_projection_mean_std_2011to2055_new\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:   \u001b[38;5;66;03m#Pickling\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(ensemble_projection_mean_std, fp)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'change_ensemble' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"freshwater_fluxes_projection_mean_boot_2011to2055_new\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mean_boot, fp)\n",
    "with open(\"freshwater_fluxes_projection_std_boot_2011to2055_new\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(std_boot, fp)\n",
    "with open(\"freshwater_fluxes_projection_pval_lin_regress_2011to2055_new\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pval_store, fp)\n",
    "\n",
    "ensemble_projection_mean_std=[change_ensemble.mean(),change_ensemble.std()]\n",
    "with open(\"freshwater_fluxes_ensemble_projection_mean_std_2011to2055_new\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(ensemble_projection_mean_std, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we need to do the same for each individual member. Now the integration technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EP_pattern_wfo_list=np.empty([34,50])\n",
    "for j in range(0,34):\n",
    "    for i in range(0,50):\n",
    "        EP_pattern_wfo_list[j,i]=np.abs((((salt_avg_list[j][i*12:(i+1)*12,:,:].mean('time')).where(np.sign(wfo.mean('time'))>0))*area.where(area.latitude<65)).sum())+np.abs((((salt_avg_list[j][i*12:(i+1)*12,:,:].mean('time')).where(np.sign(wfo.mean('time'))<0))*area.where(area.latitude<65)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "A=((np.abs(wfo.mean('time')))*area.where(area.latitude<65)).sum() #strength of wfo in this way, 9.99660706e+08 if calculated with regridded or 1.08156723e+09 if not calculated with regridded\n",
    "\n",
    "change=np.empty([3000,34])\n",
    "for j in range(0,34):\n",
    "    trend=np.empty([45])\n",
    "    p=scipy.stats.linregress(np.linspace(0,44,45), y=EP_pattern_wfo_list[j,5:50], alternative='two-sided')\n",
    "    trend=p.intercept+p.slope*np.linspace(0,44,45)\n",
    "\n",
    "    from recombinator.block_bootstrap import circular_block_bootstrap\n",
    "\n",
    "    # number of replications for bootstraps (number of resampled time-series to generate)\n",
    "    B = 3000\n",
    "\n",
    "    y_star_cb \\\n",
    "        = circular_block_bootstrap(EP_pattern_wfo_list[j,5:50]-trend, \n",
    "                                   block_length=2, \n",
    "                                   replications=B, replace=True)\n",
    "    bootstrap=[]\n",
    "    for i in range(0,B):\n",
    "        bootstrap.append(trend+y_star_cb[i,:])\n",
    "\n",
    "    for i in range(0,B):\n",
    "        change[i,j]=(bootstrap[i][42:45].mean()-bootstrap[i][0:3].mean())/A\n",
    "\n",
    "mean_boot_m2=np.empty([34])\n",
    "std_boot_m2=np.empty([34])\n",
    "for j in range(0,34):\n",
    "    mean_boot_m2[j]=change[:,j].mean()\n",
    "    std_boot_m2[j]=change[:,j].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"freshwater_fluxes_integration_mean_boot_2011to2055\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(mean_boot_m2, fp)\n",
    "with open(\"freshwater_fluxes_integration_std_boot_2011to2055\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(std_boot_m2, fp)\n",
    "with open(\"freshwater_fluxes_integration_pval_lin_regress_2011to2055\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(pval_store, fp)\n",
    "\n",
    "ensemble_projection_mean_std=[change2.mean(),change2.std()]\n",
    "with open(\"freshwater_fluxes_ensemble_integration_mean_std_2011to2055\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(ensemble_projection_mean_std, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
